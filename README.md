# Dimensionality_Reduction-Principal Component Analysis: Overview
By Sayana Varughese
A major aspect of Machine learning is Data Analysis. The performance of a model depends on the data that is used to train and test it. But working with high dimensional (muliple features) data makes it difficult to analyse or intepret or even visualise the given data[1]. Dimensionality reduction deals with transforming data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data[2]. Principal Component Analysis (PCA) is a linear dimensionality reduction approach ,based on linear algebra, that has been around for more than a 100 years and is considered as the most common technique for data compression and visualization [1]. In simple words, PCA is a method for compressing a lot of data into something that captures the essence of the original data. It takes a dataset with a lot of dimension (i.e., datasets with a lot of features) and flattens it into 2 or 3 dimensions hence making it easier to analyse the data[3]. In order to work with PCA, it is quite important to have an understanding of:

covariance,
basis and basis change,
projections,
eigenvalues,
Gaussian distributions, and
constrained optimization.
